# test_query_vectorstore_fixed.py
import os
import pickle
import faiss
import numpy as np
from dotenv import load_dotenv

# Path import ‚Äî ƒëi·ªÅu ch·ªânh n·∫øu b·∫°n ƒë·ªÉ file ·ªü ch·ªó kh√°c
from app.utils.custom_embedding import CustomEmbedding
from app.config.env_loader import logger

load_dotenv()

# -----------------------------------------------------------------------------
# C·∫§U H√åNH
# -----------------------------------------------------------------------------
VECTORSTORE_PATH = "app/vectorstore/docs_index_cosine"
INDEX_FILE = os.path.join(VECTORSTORE_PATH, "index.faiss")
PICKLE_FILE = os.path.join(VECTORSTORE_PATH, "index.pkl")


# -----------------------------------------------------------------------------
# H√ÄM H·ªñ TR·ª¢ TR√çCH XU·∫§T DOCS T·ª™ NHI·ªÄU C·∫§U TR√öC PICKLE
# -----------------------------------------------------------------------------
def _extract_from_docstore_obj(docstore):
    """
    Tr√≠ch danh s√°ch Document t·ª´ m·ªôt docstore object (LangChain docstore).
    Tr·∫£ v·ªÅ dict mapping id -> Document-like object (c√≥ attributes .page_content, .metadata).
    """
    # LangChain docstore th∆∞·ªùng c√≥ attribute _dict
    if hasattr(docstore, "_dict"):
        return docstore._dict  # dict: id -> Document
    # ƒê√¥i khi docstore ch√≠nh l√† dict
    if isinstance(docstore, dict):
        return docstore
    # N·∫øu docstore c√≥ method get_items / items
    if hasattr(docstore, "items"):
        try:
            return dict(docstore.items())
        except Exception:
            pass
    raise ValueError("Kh√¥ng th·ªÉ tr√≠ch th√¥ng tin t·ª´ docstore object.")


def _normalize_text_and_metadata_list(texts_list, metadatas_list):
    """
    ƒê·∫£m b·∫£o texts_list v√† metadatas_list t·ªìn t·∫°i v√† c√πng length.
    N·∫øu metadatas_list l√† None, t·∫°o list empty dicts.
    """
    if texts_list is None:
        texts_list = []
    if metadatas_list is None:
        metadatas_list = [{} for _ in texts_list]
    # if lengths differ, pad metadatas
    if len(metadatas_list) < len(texts_list):
        metadatas_list = list(metadatas_list) + [{}] * (len(texts_list) - len(metadatas_list))
    return texts_list, metadatas_list


# -----------------------------------------------------------------------------
# LOAD VECTORSTORE (INDEX + METADATA) - ROBUST
# -----------------------------------------------------------------------------
def load_vectorstore(index_path: str, pickle_path: str):
    """
    Load FAISS index v√† metadata t·ª´ index.pkl v·ªõi nhi·ªÅu format kh√°c nhau.
    Tr·∫£ v·ªÅ:
        - index (faiss.Index)
        - texts: list of strings aligned with index positions (index position -> texts[pos])
        - metadatas: list of metadata dicts aligned v·ªõi texts
    """
    if not os.path.exists(index_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file FAISS index t·∫°i: {index_path}")
    if not os.path.exists(pickle_path):
        raise FileNotFoundError(f"Kh√¥ng t√¨m th·∫•y file metadata index.pkl t·∫°i: {pickle_path}")

    # Load FAISS index
    index = faiss.read_index(index_path)

    # Load pickle content
    with open(pickle_path, "rb") as f:
        store_data = pickle.load(f)

    texts = None
    metadatas = None
    index_to_docstore_id = None

    # Case A: store_data is dict and uses LangChain save_local structure
    if isinstance(store_data, dict):
        # Common keys: 'docstore', 'index_to_docstore_id', 'texts', 'metadatas'
        if "texts" in store_data and isinstance(store_data["texts"], list):
            # custom manual save format
            texts = store_data["texts"]
            metadatas = store_data.get("metadatas", [{}] * len(texts))
            logger.info("Load index.pkl: found manual dict with 'texts' key.")
        elif "docstore" in store_data:
            docstore = store_data["docstore"]
            try:
                doc_dict = _extract_from_docstore_obj(docstore)
            except Exception as e:
                raise ValueError(f"C·∫•u tr√∫c docstore kh√¥ng h·ª£p l·ªá trong index.pkl: {e}")

            index_to_docstore_id = store_data.get("index_to_docstore_id", None)
            if index_to_docstore_id:
                # Build texts/metadatas aligned with index_to_docstore_id
                texts = []
                metadatas = []
                for did in index_to_docstore_id:
                    if did in doc_dict:
                        doc = doc_dict[did]
                    else:
                        # Some docstore keys may be bytes/int, try str
                        doc = doc_dict.get(str(did)) or doc_dict.get(int(did))
                    if doc is None:
                        texts.append("")
                        metadatas.append({})
                    else:
                        # Document object may be a LangChain Document or simple dict
                        if hasattr(doc, "page_content"):
                            texts.append(doc.page_content)
                            metadatas.append(getattr(doc, "metadata", {}))
                        elif isinstance(doc, dict) and "page_content" in doc:
                            texts.append(doc.get("page_content", ""))
                            metadatas.append(doc.get("metadata", {}))
                        else:
                            # fallback: stringify
                            texts.append(str(doc))
                            metadatas.append({})
                logger.info("Load index.pkl: extracted from dict docstore + index_to_docstore_id.")
            else:
                # No index mapping provided; iterate doc_dict values in insertion order
                texts = []
                metadatas = []
                for doc in doc_dict.values():
                    if hasattr(doc, "page_content"):
                        texts.append(doc.page_content)
                        metadatas.append(getattr(doc, "metadata", {}))
                    elif isinstance(doc, dict) and "page_content" in doc:
                        texts.append(doc.get("page_content", ""))
                        metadatas.append(doc.get("metadata", {}))
                    else:
                        texts.append(str(doc))
                        metadatas.append({})
                logger.info("Load index.pkl: extracted docstore dict values (no index mapping).")

    # Case B: store_data is object that wraps docstore (e.g., pickled FAISS wrapper)
    elif hasattr(store_data, "docstore"):
        try:
            docstore = store_data.docstore
            doc_dict = _extract_from_docstore_obj(docstore)
        except Exception as e:
            raise ValueError(f"C·∫•u tr√∫c pickle kh√¥ng h·ª£p l·ªá: thi·∫øu docstore ho·∫∑c docstore kh√¥ng ƒë·ªçc ƒë∆∞·ª£c: {e}")

        index_to_docstore_id = getattr(store_data, "index_to_docstore_id", None)
        if index_to_docstore_id:
            texts = []
            metadatas = []
            for did in index_to_docstore_id:
                doc = doc_dict.get(did) or doc_dict.get(str(did)) or doc_dict.get(int(did))
                if doc is None:
                    texts.append("")
                    metadatas.append({})
                else:
                    if hasattr(doc, "page_content"):
                        texts.append(doc.page_content)
                        metadatas.append(getattr(doc, "metadata", {}))
                    elif isinstance(doc, dict) and "page_content" in doc:
                        texts.append(doc.get("page_content", ""))
                        metadatas.append(doc.get("metadata", {}))
                    else:
                        texts.append(str(doc))
                        metadatas.append({})
            logger.info("Load index.pkl: extracted from object.docstore + index_to_docstore_id.")
        else:
            # fallback iterate doc_dict
            texts = []
            metadatas = []
            for doc in doc_dict.values():
                if hasattr(doc, "page_content"):
                    texts.append(doc.page_content)
                    metadatas.append(getattr(doc, "metadata", {}))
                elif isinstance(doc, dict) and "page_content" in doc:
                    texts.append(doc.get("page_content", ""))
                    metadatas.append(doc.get("metadata", {}))
                else:
                    texts.append(str(doc))
                    metadatas.append({})
            logger.info("Load index.pkl: extracted from object.docstore values (no index mapping).")

    # Case C: store_data is tuple or list - try to find docstore / mapping inside
    elif isinstance(store_data, (tuple, list)):
        found = False
        # try each element
        for part in store_data:
            if isinstance(part, dict) and "docstore" in part:
                # reuse logic for dict
                tmp = part
                docstore = tmp["docstore"]
                try:
                    doc_dict = _extract_from_docstore_obj(docstore)
                except Exception:
                    continue
                index_to_docstore_id = tmp.get("index_to_docstore_id", None)
                if index_to_docstore_id:
                    texts = []
                    metadatas = []
                    for did in index_to_docstore_id:
                        doc = doc_dict.get(did) or doc_dict.get(str(did))
                        if doc is None:
                            texts.append("")
                            metadatas.append({})
                        else:
                            if hasattr(doc, "page_content"):
                                texts.append(doc.page_content)
                                metadatas.append(getattr(doc, "metadata", {}))
                            elif isinstance(doc, dict) and "page_content" in doc:
                                texts.append(doc.get("page_content", ""))
                                metadatas.append(doc.get("metadata", {}))
                            else:
                                texts.append(str(doc))
                                metadatas.append({})
                    found = True
                    break
                else:
                    # fallback iterate doc_dict
                    texts = []
                    metadatas = []
                    for doc in doc_dict.values():
                        if hasattr(doc, "page_content"):
                            texts.append(doc.page_content)
                            metadatas.append(getattr(doc, "metadata", {}))
                        elif isinstance(doc, dict) and "page_content" in doc:
                            texts.append(doc.get("page_content", ""))
                            metadatas.append(doc.get("metadata", {}))
                        else:
                            texts.append(str(doc))
                            metadatas.append({})
                    found = True
                    break
            # if part itself has docstore attribute
            if hasattr(part, "docstore"):
                try:
                    doc_dict = _extract_from_docstore_obj(part.docstore)
                except Exception:
                    continue
                index_to_docstore_id = getattr(part, "index_to_docstore_id", None)
                if index_to_docstore_id:
                    texts = []
                    metadatas = []
                    for did in index_to_docstore_id:
                        doc = doc_dict.get(did) or doc_dict.get(str(did))
                        if doc is None:
                            texts.append("")
                            metadatas.append({})
                        else:
                            if hasattr(doc, "page_content"):
                                texts.append(doc.page_content)
                                metadatas.append(getattr(doc, "metadata", {}))
                            elif isinstance(doc, dict) and "page_content" in doc:
                                texts.append(doc.get("page_content", ""))
                                metadatas.append(doc.get("metadata", {}))
                            else:
                                texts.append(str(doc))
                                metadatas.append({})
                    found = True
                    break
                else:
                    texts = []
                    metadatas = []
                    for doc in doc_dict.values():
                        if hasattr(doc, "page_content"):
                            texts.append(doc.page_content)
                            metadatas.append(getattr(doc, "metadata", {}))
                        elif isinstance(doc, dict) and "page_content" in doc:
                            texts.append(doc.get("page_content", ""))
                            metadatas.append(doc.get("metadata", {}))
                        else:
                            texts.append(str(doc))
                            metadatas.append({})
                    found = True
                    break
        if not found:
            raise ValueError("C·∫•u tr√∫c pickle l√† tuple/list nh∆∞ng kh√¥ng t√¨m th·∫•y docstore hay ƒë·ªãnh d·∫°ng mong ƒë·ª£i.")
    else:
        raise ValueError("Kh√¥ng nh·∫≠n d·∫°ng ƒë∆∞·ª£c c·∫•u tr√∫c file index.pkl. Vui l√≤ng ki·ªÉm tra ƒë·ªãnh d·∫°ng file.")

    # Ensure lists exist and lengths are consistent
    texts, metadatas = _normalize_text_and_metadata_list(texts, metadatas)

    # If index_to_docstore_id exists and length mismatch with faiss index, log a warning
    try:
        ntotal = index.ntotal
    except Exception:
        ntotal = None

    if ntotal is not None and len(texts) != ntotal:
        logger.warning(
            f"S·ªë l∆∞·ª£ng vectors trong FAISS ({ntotal}) v√† s·ªë texts l·∫•y ƒë∆∞·ª£c ({len(texts)}) kh√¥ng kh·ªõp. "
            "S·ª± kh√¥ng kh·ªõp c√≥ th·ªÉ do l∆∞u index kh√°c c√°ch. K·∫øt qu·∫£ truy v·∫•n s·∫Ω c·ªë g·∫Øng s·ª≠ d·ª•ng ch·ªâ s·ªë tr·∫£ v·ªÅ t·ª´ FAISS."
        )

    logger.info(f"‚úÖ ƒê√£ load vectorstore: {len(texts)} items (FAISS ntotal={ntotal}).")
    return index, texts, metadatas


# -----------------------------------------------------------------------------
# TRUY V·∫§N VECTORSTORE B·∫∞NG EMBED_QUERY()
# -----------------------------------------------------------------------------
def query_vectorstore(user_query: str, top_k: int = 5, selected_model: str = "text-embedding"):
    """
    T√≠nh embedding cho c√¢u h·ªèi b·∫±ng CustomEmbedding.embed_query()
    r·ªìi truy v·∫•n FAISS index v√† in k·∫øt qu·∫£ (score, content, metadata).
    """
    # Kh·ªüi t·∫°o embedding client (d√πng bi·∫øn m√¥i tr∆∞·ªùng n·∫øu kh√¥ng truy·ªÅn tham s·ªë)
    embedding_model = CustomEmbedding(selected_embedding_model=selected_model, use_cosine=True)

    # T·∫°o embedding query
    query_vector = embedding_model.embed_query(user_query)
    print(f"query_vector len: {len(query_vector)}")
    
    query_vector = np.array(query_vector, dtype=np.float32).reshape(1, -1)
    print(f"query_vector len [2]: {len(query_vector)}")
    
    # Load index + metadata
    index, texts, metadatas = load_vectorstore(INDEX_FILE, PICKLE_FILE)

    # Th·ª±c hi·ªán t√¨m ki·∫øm
    scores, indices = index.search(query_vector, top_k)

    print(f"\nüîπ K·∫øt qu·∫£ Top-{top_k} cho c√¢u h·ªèi: {user_query}\n")
    for rank, (score, idx) in enumerate(zip(scores[0], indices[0]), start=1):
        # FAISS tr·∫£ -1 n·∫øu kh√¥ng t√¨m th·∫•y
        if idx == -1:
            continue
        # N·∫øu idx v∆∞·ª£t qu√° texts length, in c·∫£nh b√°o v√† skip
        if idx >= len(texts):
            logger.warning(f"Index tr·∫£ v·ªÅ ({idx}) v∆∞·ª£t qu√° chi·ªÅu d√†i texts ({len(texts)}). B·ªè qua.")
            continue
        content = texts[idx]
        metadata = metadatas[idx] if metadatas and idx < len(metadatas) else {}
        print(f"--- Document {rank} ---")
        print(f"Score: {score:.6f}")
        print(f"Content: {content[:1000]}{'...' if len(content) > 1000 else ''}")
        print(f"Metadata: {metadata}\n")


# -----------------------------------------------------------------------------
# MAIN
# -----------------------------------------------------------------------------
if __name__ == "__main__":
    try:
        query = input("Nh·∫≠p c√¢u h·ªèi c·ªßa b·∫°n: ").strip()
        if not query:
            print("‚ö†Ô∏è C√¢u h·ªèi kh√¥ng ƒë∆∞·ª£c ƒë·ªÉ tr·ªëng.")
        else:
            # b·∫°n c√≥ th·ªÉ ƒë·ªïi selected_model="cohere-multilingual" n·∫øu index ƒë∆∞·ª£c build b·∫±ng cohere
            query_vectorstore(query, top_k=5, selected_model="text-embedding")
    except Exception as e:
        logger.error(f"‚ùå L·ªói khi ch·∫°y truy v·∫•n: {e}")
        raise
